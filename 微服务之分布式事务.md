# 分布式事务解决方案

## 问题产生

在微服务环境下，不同业务会拆成不同的服务，每个服务都有独立的数据库

## CAP原理

C - consistency 一致性：数据从主节点同步到所有节点

A - availability 可用性：主节点挂了，再选主

P - partition 分区容错性：分区因为网络问题出现问题

CAP 没办法同时满足，只能 CP（zk） 或 AP（Eureka，redis）

## 常见的解决方案

### 2PC

即二阶段提交，先准备，再提交

![image-20201118203208322](C:\Users\peng\AppData\Roaming\Typora\typora-user-images\image-20201118203208322.png)

undolog：记录上修改前的数据，中途失败能回滚

存在问题：

1. 阻塞，效率低
2. 协调者挂了，拉闸
3. 参与者挂了（部分回滚成功了，导致脏读）

#### 3PC

先检查，再准备，最后提交，参与者挂了早早就知道，不用记录和回滚了，解决了问题3，但本质上还是垃圾

### 消息事务 MQ

![image-20201118204052616](C:\Users\peng\AppData\Roaming\Typora\typora-user-images\image-20201118204052616.png)

主从多节点，异步，存到内存

存在问题：

1. 并发很高时，消息泛滥，内存容易爆炸，性能下降

### TCC 事务

try - confirm - cancel

是一个规范，需要自己定制代码实现

![image-20201118204534634](C:\Users\peng\AppData\Roaming\Typora\typora-user-images\image-20201118204534634.png)

![image-20201118204549081](C:\Users\peng\AppData\Roaming\Typora\typora-user-images\image-20201118204549081.png)

### seata

分支事务，全局事务，都由 TC(seata服务器) 管理

如果失败了，其他事务批量的异步的回滚

![image-20201118205012615](C:\Users\peng\AppData\Roaming\Typora\typora-user-images\image-20201118205012615.png)